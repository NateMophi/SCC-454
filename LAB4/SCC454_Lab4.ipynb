{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHla+utfayVf47tPlNbk9d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NateMophi/SCC-454/blob/main/LAB4/SCC454_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSsvJ7MWFlJh",
        "outputId": "cf343405-9034-45ae-93ca-dbf2cda1112e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install pyspark==3.5.0 -q\n",
        "!pip install sentence-transformers -q\n",
        "!pip install numpy pandas -q\n",
        "\n",
        "# Install Java (Spark requires Java)\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Set Java environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "print(\"All packages installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIY_GNL1Fr6R",
        "outputId": "0b7fefa0-57ca-41b5-ed44-5be7d41fbee3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 1.0.1 requires pyspark[connect]~=4.0.0, but you have pyspark 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mAll packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import hashlib\n",
        "from typing import List, Set, Tuple"
      ],
      "metadata": {
        "id": "SpNBUmiWHS8r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import(\n",
        "    col, udf, explode, array, lit, collect_list, size, lower, regexp_replace, split, monotonically_increasing_id, struct,\n",
        "    when, coalesce, broadcast\n",
        ")\n",
        "\n",
        "from pyspark.sql.types import(\n",
        "    ArrayType, StringType, IntegerType, FloatType, StructType, StructField,\n",
        "    DoubleType\n",
        ")\n",
        "\n",
        "from pyspark.ml.feature import (\n",
        "    HashingTF, CountVectorizer, MinHashLSH,\n",
        "    Tokenizer, StopWordsRemover, NGram\n",
        ")\n",
        "\n",
        "from pyspark.ml.linalg import Vectors, SparseVector, VectorUDT\n",
        "from pyspark.ml import Pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "mARe4OcvGT1-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session for LSH Operations\n",
        "spark = SparkSession.builder.appName(\"SCC454-LocalitySensitiveHashing\")\\\n",
        ".config(\"spark.driver.memory\", \"4g\")\\\n",
        ".config(\"spark.sql.shuffle.partitions\", \"8\")\\\n",
        ".config(\"spark.ui.port\", \"4050\")\\\n",
        ".getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"App Name: {spark.sparkContext.appName}\")\n",
        "print(\"\\nSpark Session Ready for LSH ops!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VDkHZRkGTzw",
        "outputId": "c30706b4-9654-44c5-c8ab-4e6a47df1e93"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Version: 3.5.0\n",
            "App Name: SCC454-LocalitySensitiveHashing\n",
            "\n",
            "Spark Session Ready for LSH ops!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAMPLE DATASET"
      ],
      "metadata": {
        "id": "gjZRYGfyK5Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample document corpus with varying degrees of similarity\n",
        "documents = [\n",
        "    (0, \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\"),\n",
        "    (1, \"Artificial intelligence and machine learning allow computers to learn from data automatically.\"),\n",
        "    (2, \"Deep learning is a type of machine learning using neural networks with many layers.\"),\n",
        "    (3, \"The weather today is sunny with a high of 25 degrees celsius.\"),\n",
        "    (4, \"Today's weather forecast shows sunny skies and temperatures around 25 degrees.\"),\n",
        "    (5, \"Natural language processing helps computers understand human language.\"),\n",
        "    (6, \"NLP enables machines to process and understand natural human language.\"),\n",
        "    (7, \"Python is a popular programming language for data science and machine learning.\"),\n",
        "    (8, \"Data science often uses Python programming for machine learning applications.\"),\n",
        "    (9, \"The cat sat on the mat and watched the birds outside the window.\"),\n",
        "    (10, \"A small cat was sitting on a mat, watching birds through the window.\"),\n",
        "    (11, \"Apache Spark provides distributed computing for big data processing.\"),\n",
        "    (12, \"Big data processing is made efficient through distributed computing with Spark.\"),\n",
        "    (13, \"Locality sensitive hashing enables fast approximate nearest neighbor search.\"),\n",
        "    (14, \"LSH provides fast approximate nearest neighbor queries using hashing techniques.\"),\n",
        "    (15, \"The restaurant serves delicious Italian pasta and fresh salads daily.\"),\n",
        "]\n",
        "\n",
        "df_docs = spark.createDataFrame(documents, [\"id\", \"text\"])\n",
        "\n",
        "print(\"Sample Document Corpus:\")\n",
        "df_docs.show(truncate=60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rofcOP7KGTxS",
        "outputId": "a099010d-32b8-4b5e-8ae3-212f9253744f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Document Corpus:\n",
            "+---+------------------------------------------------------------+\n",
            "| id|                                                        text|\n",
            "+---+------------------------------------------------------------+\n",
            "|  0|Machine learning is a subset of artificial intelligence t...|\n",
            "|  1|Artificial intelligence and machine learning allow comput...|\n",
            "|  2|Deep learning is a type of machine learning using neural ...|\n",
            "|  3|The weather today is sunny with a high of 25 degrees cels...|\n",
            "|  4|Today's weather forecast shows sunny skies and temperatur...|\n",
            "|  5|Natural language processing helps computers understand hu...|\n",
            "|  6|NLP enables machines to process and understand natural hu...|\n",
            "|  7|Python is a popular programming language for data science...|\n",
            "|  8|Data science often uses Python programming for machine le...|\n",
            "|  9|The cat sat on the mat and watched the birds outside the ...|\n",
            "| 10|A small cat was sitting on a mat, watching birds through ...|\n",
            "| 11|Apache Spark provides distributed computing for big data ...|\n",
            "| 12|Big data processing is made efficient through distributed...|\n",
            "| 13|Locality sensitive hashing enables fast approximate neare...|\n",
            "| 14|LSH provides fast approximate nearest neighbor queries us...|\n",
            "| 15|The restaurant serves delicious Italian pasta and fresh s...|\n",
            "+---+------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DOCUMENT SHINGLING**"
      ],
      "metadata": {
        "id": "l679jTqOLdGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 2: Document Shingling\n",
        "---\n",
        "\n",
        "## 2.1 Understanding Shingling\n",
        "\n",
        "**What is Shingling?**\n",
        "Shingling converts documents into sets of contiguous subsequences (shingles). This allows us to measure document similarity by comparing these sets.\n",
        "\n",
        "**Types of Shingles:**\n",
        "\n",
        "| Type | Description | Example (\"hello world\") |\n",
        "|------|-------------|-------------------------|\n",
        "| Character k-shingles | Contiguous k characters | {\"hel\", \"ell\", \"llo\", \"lo \", \"o w\", ...} |\n",
        "| Word n-grams | Contiguous n words | {\"hello world\"} for n=2 |\n",
        "\n",
        "**Choosing Shingle Size:**\n",
        "- Too small: High overlap even for dissimilar documents\n",
        "- Too large: Low overlap even for similar documents\n",
        "- Rule of thumb: k=5-9 for characters, n=2-4 for words"
      ],
      "metadata": {
        "id": "fHJQ7zqQMlkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CHARACTER SHINGLES\n",
        "def get_char_shingles(text:str, k:int =5):\n",
        "  \"\"\"Generate character k-shingles from text\"\"\"\n",
        "  text = \" \".join(text.lower().split())\n",
        "\n",
        "  # Generate Shingles\n",
        "  shingles = [text[i:i+k] for i in range(len(text) - k + 1)]\n",
        "  return shingles\n",
        "\n",
        "# Example\n",
        "sample_text = \"Hello World\"\n",
        "char_shingles = get_char_shingles(sample_text, k=5)\n",
        "print(f\"Text: '{sample_text}'\")\n",
        "print(f\"5-character shingles: {char_shingles}\")\n",
        "print(f\"Number of shingles: {len(char_shingles)}\")\n",
        "print(f\"Unique shingles: {len(set(char_shingles))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nki2oPJGTqs",
        "outputId": "a7c78a3d-430f-44fd-b095-7f2711d1d196"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'Hello World'\n",
            "5-character shingles: ['hello', 'ello ', 'llo w', 'lo wo', 'o wor', ' worl', 'world']\n",
            "Number of shingles: 7\n",
            "Unique shingles: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WORD SHINGLES (n-grams)\n",
        "def get_word_shingles(text: str, n:int=3)-> List[str]:\n",
        "  words = text.lower().split()\n",
        "  shingles = [\" \".join(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
        "  return shingles\n",
        "\n",
        "  # Example\n",
        "sample_text = \"Machine learning is a subset of artificial intelligence\"\n",
        "word_shingles = get_word_shingles(sample_text, n=3)\n",
        "print(f\"Text: '{sample_text}'\")\n",
        "print(f\"\\n3-word shingles:\")\n",
        "for i, shingle in enumerate(word_shingles):\n",
        "    print(f\"  {i+1}. '{shingle}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSYw0I6zGToB",
        "outputId": "b485125b-370b-4a02-bee5-8791c6ee4696"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'Machine learning is a subset of artificial intelligence'\n",
            "\n",
            "3-word shingles:\n",
            "  1. 'machine learning is'\n",
            "  2. 'learning is a'\n",
            "  3. 'is a subset'\n",
            "  4. 'a subset of'\n",
            "  5. 'subset of artificial'\n",
            "  6. 'of artificial intelligence'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shingling in SPARK\n",
        "# Method 1: Using Spark's NGram transformer\n",
        "def _create_shingle_pipeline(n_values=[2, 3, 4]):\n",
        "  \"\"\"Create a pipeline for generating word n-grams of multiple sizes \"\"\"\n",
        "  from pyspark.sql.functions import concat_ws, flatten\n",
        "\n",
        "  # Tokenizer\n",
        "  tokenizer = Tokenizer(inputCol=\"text_clean\", outputCol=\"words\")\n",
        "\n",
        "  # Create NGram transformers for each n\n",
        "  ngram_transformers =  []\n",
        "  for n in n_values:\n",
        "    ngram = NGram(n=n, inputCol=\"words\", outputCol=f\"ngrams_{n}\")\n",
        "    ngram_transformers.append(ngram)\n",
        "\n",
        "  return tokenizer, ngram_transformers\n",
        "\n",
        "# Preprocessing Text\n",
        "\n",
        "df_clean = df_docs.withColumn(\n",
        "    \"text_clean\",\n",
        "    lower(regexp_replace(col(\"text\"), r\"[^a-zA-Z\\s]\", \"\"))\n",
        ")\n",
        "\n",
        "# Apply Tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"text_clean\", outputCol=\"words\")\n",
        "df_tokenized = tokenizer.transform(df_clean)\n",
        "\n",
        "# Applying n-grams transforemers for n = 2,3,4\n",
        "ngram_2 = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams_2\")\n",
        "ngram_3 = NGram(n=3, inputCol=\"words\", outputCol=\"ngrams_3\")\n",
        "ngram_4 = NGram(n=4, inputCol=\"words\", outputCol=\"ngrams_4\")\n",
        "\n",
        "df_shingles = ngram_2.transform(df_tokenized)\n",
        "df_shingles = ngram_3.transform(df_shingles)\n",
        "df_shingles = ngram_4.transform(df_shingles)\n",
        "\n",
        "print(\"Document with word shingles (n=2,3,4):\")\n",
        "df_shingles.select(\"id\", \"ngrams_2\", \"ngrams_3\").show(5, truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj8npgsOGTlZ",
        "outputId": "89c973b8-4ec1-4221-aa0d-21e7b02cb6a0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document with word shingles (n=2,3,4):\n",
            "+---+--------------------------------------------------+--------------------------------------------------+\n",
            "| id|                                          ngrams_2|                                          ngrams_3|\n",
            "+---+--------------------------------------------------+--------------------------------------------------+\n",
            "|  0|[machine learning, learning is, is a, a subset,...|[machine learning is, learning is a, is a subse...|\n",
            "|  1|[artificial intelligence, intelligence and, and...|[artificial intelligence and, intelligence and ...|\n",
            "|  2|[deep learning, learning is, is a, a type, type...|[deep learning is, learning is a, is a type, a ...|\n",
            "|  3|[the weather, weather today, today is, is sunny...|[the weather today, weather today is, today is ...|\n",
            "|  4|[todays weather, weather forecast, forecast sho...|[todays weather forecast, weather forecast show...|\n",
            "+---+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: Combine all n-grams into a single sheet\n",
        "from pyspark.sql.functions import concat, array_union, array_distinct\n",
        "\n",
        "# Combine 2-grams, 3-grams and 4-grams into a single shingle set\n",
        "df_combined_shingles = df_shingles.withColumn(\n",
        "    \"all_shingles\",\n",
        "    array_distinct(\n",
        "        concat(col(\"ngrams_2\"), col(\"ngrams_3\"), col(\"ngrams_4\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Combined Shingless (2-4 grams):\")\n",
        "df_combined_shingles.select(\"id\", \"all_shingles\").show(3, truncate=80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF8TmQAXUCPO",
        "outputId": "05386f0e-6545-43ac-b15b-e90f91734b32"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Shingless (2-4 grams):\n",
            "+---+--------------------------------------------------------------------------------+\n",
            "| id|                                                                    all_shingles|\n",
            "+---+--------------------------------------------------------------------------------+\n",
            "|  0|[machine learning, learning is, is a, a subset, subset of, of artificial, art...|\n",
            "|  1|[artificial intelligence, intelligence and, and machine, machine learning, le...|\n",
            "|  2|[deep learning, learning is, is a, a type, type of, of machine, machine learn...|\n",
            "+---+--------------------------------------------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jaccard similarity from shingle sets\n",
        "def jaccard_similarity(set1: set, set2: set) -> float:\n",
        "    \"\"\"Calculate Jaccard similarity between two sets.\"\"\"\n",
        "    if not set1 or not set2:\n",
        "        return 0.0\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "    # Get shingles for comparison\n",
        "shingle_data = df_combined_shingles.select(\"id\", \"text\", \"all_shingles\").collect()\n",
        "\n",
        "# Compare a few document pairs\n",
        "pairs_to_compare = [(0, 1), (0, 2), (0, 3), (3, 4), (9, 10), (0, 15)]\n",
        "\n",
        "print(\"Jaccard Similarity Analysis:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, j in pairs_to_compare:\n",
        "    doc_i = next(row for row in shingle_data if row.id == i)\n",
        "    doc_j = next(row for row in shingle_data if row.id == j)\n",
        "\n",
        "    set_i = set(doc_i.all_shingles)\n",
        "    set_j = set(doc_j.all_shingles)\n",
        "\n",
        "    jaccard = jaccard_similarity(set_i, set_j)\n",
        "\n",
        "    print(f\"\\nDocs {i} vs {j}: Jaccard = {jaccard:.4f}\")\n",
        "    print(f\"  Doc {i}: {doc_i.text[:60]}...\")\n",
        "    print(f\"  Doc {j}: {doc_j.text[:60]}...\")\n",
        "    print(f\"  Shingles: {len(set_i)} vs {len(set_j)}, Overlap: {len(set_i & set_j)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYXNzL7a6WRj",
        "outputId": "9f2b5cbc-d930-4539-c00c-d7cf12989305"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity Analysis:\n",
            "======================================================================\n",
            "\n",
            "Docs 0 vs 1: Jaccard = 0.1311\n",
            "  Doc 0: Machine learning is a subset of artificial intelligence that...\n",
            "  Doc 1: Artificial intelligence and machine learning allow computers...\n",
            "  Shingles: 39 vs 30, Overlap: 8\n",
            "\n",
            "Docs 0 vs 2: Jaccard = 0.0563\n",
            "  Doc 0: Machine learning is a subset of artificial intelligence that...\n",
            "  Doc 2: Deep learning is a type of machine learning using neural net...\n",
            "  Shingles: 39 vs 36, Overlap: 4\n",
            "\n",
            "Docs 0 vs 3: Jaccard = 0.0000\n",
            "  Doc 0: Machine learning is a subset of artificial intelligence that...\n",
            "  Doc 3: The weather today is sunny with a high of 25 degrees celsius...\n",
            "  Shingles: 39 vs 30, Overlap: 0\n",
            "\n",
            "Docs 3 vs 4: Jaccard = 0.0179\n",
            "  Doc 3: The weather today is sunny with a high of 25 degrees celsius...\n",
            "  Doc 4: Today's weather forecast shows sunny skies and temperatures ...\n",
            "  Shingles: 30 vs 27, Overlap: 1\n",
            "\n",
            "Docs 9 vs 10: Jaccard = 0.0154\n",
            "  Doc 9: The cat sat on the mat and watched the birds outside the win...\n",
            "  Doc 10: A small cat was sitting on a mat, watching birds through the...\n",
            "  Shingles: 33 vs 33, Overlap: 1\n",
            "\n",
            "Docs 0 vs 15: Jaccard = 0.0000\n",
            "  Doc 0: Machine learning is a subset of artificial intelligence that...\n",
            "  Doc 15: The restaurant serves delicious Italian pasta and fresh sala...\n",
            "  Shingles: 39 vs 24, Overlap: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 3: MinHash Fundamentals\n",
        "---\n",
        "\n",
        "## 3.1 The Jaccard Similarity Problem\n",
        "\n",
        "Computing Jaccard similarity directly has limitations:\n",
        "- Requires storing and comparing full sets\n",
        "- Memory-intensive for large vocabularies\n",
        "- O(n) per comparison where n is set size\n",
        "\n",
        "**MinHash Solution:**\n",
        "Create compact \"signatures\" that preserve similarity information.\n",
        "\n",
        "## 3.2 The MinHash Algorithm\n",
        "\n",
        "**Key Insight:**\n",
        "For any random permutation π of the universal set:\n",
        "```\n",
        "P[min(π(A)) = min(π(B))] = Jaccard(A, B)\n",
        "```\n",
        "\n",
        "**Algorithm:**\n",
        "1. Choose k random hash functions (simulating permutations)\n",
        "2. For each document, compute k minimum hash values\n",
        "3. The k values form the \"MinHash signature\"\n",
        "4. Signature similarity estimates Jaccard similarity"
      ],
      "metadata": {
        "id": "By5ThtShtj8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MinHasher:\n",
        "\n",
        "  def __init__(self, num_hashes:int= 100, seed:int= 42):\n",
        "    \"\"\"\n",
        "    Initialize MinHasher with num_hashes hash functions.\n",
        "    Args:\n",
        "      num_hashes: Number of hash functions (signature length)\n",
        "      seed: Random seed for reproducibility\n",
        "      \"\"\"\n",
        "    self.num_hashes =  num_hashes\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "    # Gen random coeffs for # function\n",
        "    # h(x) = (a*x + b) mod c\n",
        "\n",
        "    self.max_hash = 2**32 - 1\n",
        "    self.a = np.random.randint(1, self.max_hashm, size=num_hashes)\n",
        "    self.b = np.random.randint(0, self.max_hash, size=num_hashes)\n",
        "    self.c = 4294967311\n",
        "\n",
        "  def _hash_element(self, element:str)-> int:\n",
        "    \"\"\"Hash a string element to an integer.\"\"\"\n",
        "    return int(hashlib.md5(element.encode()).hexdigest(), 16) % self.max_hash\n",
        "\n",
        "  def get_signature(self, shingle_set: set) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate MinHash signature for a set of shingles.\n",
        "\n",
        "        Args:\n",
        "            shingle_set: Set of string shingles\n",
        "\n",
        "        Returns:\n",
        "            MinHash signature array of length num_hashes\n",
        "        \"\"\"\n",
        "        # Initialize signature with infinity\n",
        "        signature = np.full(self.num_hashes, np.inf)\n",
        "\n",
        "        for shingle in shingle_set:\n",
        "            # Hash the shingle to an integer\n",
        "            shingle_hash = self._hash_element(shingle)\n",
        "\n",
        "            # Apply all hash functions and keep minimum\n",
        "            hash_values = (self.a * shingle_hash + self.b) % self.c\n",
        "            signature = np.minimum(signature, hash_values)\n",
        "\n",
        "        return signature.astype(np.int64)\n",
        "\n",
        "    def estimate_similarity(self, sig1: np.ndarray, sig2: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Estimate Jaccard similarity from MinHash signatures.\n",
        "\n",
        "        Returns:\n",
        "            Estimated Jaccard similarity\n",
        "        \"\"\"\n",
        "        return np.mean(sig1 == sig2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gNJZoUN86WO3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSEASBLC6WK5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0hGqkMJPWzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRHReaSHPWtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qd-IIJ0VPWnd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}