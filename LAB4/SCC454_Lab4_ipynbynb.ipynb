{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO563yd3gHfrdfoE2ZUA91",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NateMophi/SCC-454/blob/main/LAB4/SCC454_Lab4_ipynbynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSsvJ7MWFlJh",
        "outputId": "80a237ec-df14-4856-d736-352625db1a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install pyspark==3.5.0 -q\n",
        "!pip install sentence-transformers -q\n",
        "!pip install numpy pandas -q\n",
        "\n",
        "# Install Java (Spark requires Java)\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Set Java environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "print(\"All packages installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIY_GNL1Fr6R",
        "outputId": "36801b19-a564-49e4-b4e8-1bfafcd46e00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 1.0.1 requires pyspark[connect]~=4.0.0, but you have pyspark 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mAll packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import hashlib\n",
        "from typing import List, Set, Tuple"
      ],
      "metadata": {
        "id": "SpNBUmiWHS8r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import(\n",
        "    col, udf, explode, array, lit, collect_list, size, lower, regexp_replace, split, monotonically_increasing_id, struct,\n",
        "    when, coalesce, broadcast\n",
        ")\n",
        "\n",
        "from pyspark.sql.types import(\n",
        "    ArrayType, StringType, IntegerType, FloatType, StructType, StructField,\n",
        "    DoubleType\n",
        ")\n",
        "\n",
        "from pyspark.ml.feature import (\n",
        "    HashingTF, CountVectorizer, MinHashLSH,\n",
        "    Tokenizer, StopWordsRemover, NGram\n",
        ")\n",
        "\n",
        "from pyspark.ml.linalg import Vectors, SparseVector, VectorUDT\n",
        "from pyspark.ml import Pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "mARe4OcvGT1-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Session for LSH Operations\n",
        "spark = SparkSession.builder.appName(\"SCC454-LocalitySensitiveHashing\")\\\n",
        ".config(\"spark.driver.memory\", \"4g\")\\\n",
        ".config(\"spark.sql.shuffle.partitions\", \"8\")\\\n",
        ".config(\"spark.ui.port\", \"4050\")\\\n",
        ".getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"App Name: {spark.sparkContext.appName}\")\n",
        "print(\"\\nSpark Session Ready for LSH ops!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VDkHZRkGTzw",
        "outputId": "79a94a40-757b-479e-8280-732294b44f39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Version: 3.5.0\n",
            "App Name: SCC454-LocalitySensitiveHashing\n",
            "\n",
            "Spark Session Ready for LSH ops!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAMPLE DATASET"
      ],
      "metadata": {
        "id": "gjZRYGfyK5Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample document corpus with varying degrees of similarity\n",
        "documents = [\n",
        "    (0, \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\"),\n",
        "    (1, \"Artificial intelligence and machine learning allow computers to learn from data automatically.\"),\n",
        "    (2, \"Deep learning is a type of machine learning using neural networks with many layers.\"),\n",
        "    (3, \"The weather today is sunny with a high of 25 degrees celsius.\"),\n",
        "    (4, \"Today's weather forecast shows sunny skies and temperatures around 25 degrees.\"),\n",
        "    (5, \"Natural language processing helps computers understand human language.\"),\n",
        "    (6, \"NLP enables machines to process and understand natural human language.\"),\n",
        "    (7, \"Python is a popular programming language for data science and machine learning.\"),\n",
        "    (8, \"Data science often uses Python programming for machine learning applications.\"),\n",
        "    (9, \"The cat sat on the mat and watched the birds outside the window.\"),\n",
        "    (10, \"A small cat was sitting on a mat, watching birds through the window.\"),\n",
        "    (11, \"Apache Spark provides distributed computing for big data processing.\"),\n",
        "    (12, \"Big data processing is made efficient through distributed computing with Spark.\"),\n",
        "    (13, \"Locality sensitive hashing enables fast approximate nearest neighbor search.\"),\n",
        "    (14, \"LSH provides fast approximate nearest neighbor queries using hashing techniques.\"),\n",
        "    (15, \"The restaurant serves delicious Italian pasta and fresh salads daily.\"),\n",
        "]\n",
        "\n",
        "df_docs = spark.createDataFrame(documents, [\"id\", \"text\"])\n",
        "\n",
        "print(\"Sample Document Corpus:\")\n",
        "df_docs.show(truncate=60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rofcOP7KGTxS",
        "outputId": "fb77d27a-e5b9-4854-c883-545dd9ec0588"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Document Corpus:\n",
            "+---+------------------------------------------------------------+\n",
            "| id|                                                        text|\n",
            "+---+------------------------------------------------------------+\n",
            "|  0|Machine learning is a subset of artificial intelligence t...|\n",
            "|  1|Artificial intelligence and machine learning allow comput...|\n",
            "|  2|Deep learning is a type of machine learning using neural ...|\n",
            "|  3|The weather today is sunny with a high of 25 degrees cels...|\n",
            "|  4|Today's weather forecast shows sunny skies and temperatur...|\n",
            "|  5|Natural language processing helps computers understand hu...|\n",
            "|  6|NLP enables machines to process and understand natural hu...|\n",
            "|  7|Python is a popular programming language for data science...|\n",
            "|  8|Data science often uses Python programming for machine le...|\n",
            "|  9|The cat sat on the mat and watched the birds outside the ...|\n",
            "| 10|A small cat was sitting on a mat, watching birds through ...|\n",
            "| 11|Apache Spark provides distributed computing for big data ...|\n",
            "| 12|Big data processing is made efficient through distributed...|\n",
            "| 13|Locality sensitive hashing enables fast approximate neare...|\n",
            "| 14|LSH provides fast approximate nearest neighbor queries us...|\n",
            "| 15|The restaurant serves delicious Italian pasta and fresh s...|\n",
            "+---+------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DOCUMENT SHINGLING**"
      ],
      "metadata": {
        "id": "l679jTqOLdGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 2: Document Shingling\n",
        "---\n",
        "\n",
        "## 2.1 Understanding Shingling\n",
        "\n",
        "**What is Shingling?**\n",
        "Shingling converts documents into sets of contiguous subsequences (shingles). This allows us to measure document similarity by comparing these sets.\n",
        "\n",
        "**Types of Shingles:**\n",
        "\n",
        "| Type | Description | Example (\"hello world\") |\n",
        "|------|-------------|-------------------------|\n",
        "| Character k-shingles | Contiguous k characters | {\"hel\", \"ell\", \"llo\", \"lo \", \"o w\", ...} |\n",
        "| Word n-grams | Contiguous n words | {\"hello world\"} for n=2 |\n",
        "\n",
        "**Choosing Shingle Size:**\n",
        "- Too small: High overlap even for dissimilar documents\n",
        "- Too large: Low overlap even for similar documents\n",
        "- Rule of thumb: k=5-9 for characters, n=2-4 for words"
      ],
      "metadata": {
        "id": "fHJQ7zqQMlkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CHARACTER SHINGLES\n",
        "def get_char_shingles(text:str, k:int =5):\n",
        "  \"\"\"Generate character k-shingles from text\"\"\"\n",
        "  text = \" \".join(text.lower().split())\n",
        "\n",
        "  # Generate Shingles\n",
        "  shingles = [text[i:i+k] for i in range(len(text) - k + 1)]\n",
        "  return shingles\n",
        "\n",
        "# Example\n",
        "sample_text = \"Hello World\"\n",
        "char_shingles = get_char_shingles(sample_text, k=5)\n",
        "print(f\"Text: '{sample_text}'\")\n",
        "print(f\"5-character shingles: {char_shingles}\")\n",
        "print(f\"Number of shingles: {len(char_shingles)}\")\n",
        "print(f\"Unique shingles: {len(set(char_shingles))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nki2oPJGTqs",
        "outputId": "916c56f3-10c5-4374-f4ea-2b35dd10c84a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'Hello World'\n",
            "5-character shingles: ['hello', 'ello ', 'llo w', 'lo wo', 'o wor', ' worl', 'world']\n",
            "Number of shingles: 7\n",
            "Unique shingles: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WORD SHINGLES (n-grams)\n",
        "def get_word_shingles(text: str, n:int=3)-> List[str]:\n",
        "  words = text.lower().split()\n",
        "  shingles = [\" \".join(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
        "  return shingles\n",
        "\n",
        "  # Example\n",
        "sample_text = \"Machine learning is a subset of artificial intelligence\"\n",
        "word_shingles = get_word_shingles(sample_text, n=3)\n",
        "print(f\"Text: '{sample_text}'\")\n",
        "print(f\"\\n3-word shingles:\")\n",
        "for i, shingle in enumerate(word_shingles):\n",
        "    print(f\"  {i+1}. '{shingle}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSYw0I6zGToB",
        "outputId": "b58ef8fb-052f-46b9-e78a-70ced5576bce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'Machine learning is a subset of artificial intelligence'\n",
            "\n",
            "3-word shingles:\n",
            "  1. 'machine learning is'\n",
            "  2. 'learning is a'\n",
            "  3. 'is a subset'\n",
            "  4. 'a subset of'\n",
            "  5. 'subset of artificial'\n",
            "  6. 'of artificial intelligence'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shingling in SPARK\n",
        "# Method 1: Using Spark's NGram transformer\n",
        "def _create_shingle_pipeline(n_values=[2, 3, 4]):\n",
        "  \"\"\"Create a pipeline for generating word n-grams of multiple sizes \"\"\"\n",
        "  from pyspark.sql.functions import concat_ws, flatten\n",
        "\n",
        "  # Tokenizer\n",
        "  tokenizer = Tokenizer(inputCol=\"text_clean\", outputCol=\"words\")\n",
        "\n",
        "  # Create NGram transformers for each n\n",
        "  ngram_transformers =  []\n",
        "  for n in n_values:\n",
        "    ngram = NGram(n=n, inputCol=\"words\", outputCol=f\"ngrams_{n}\")\n",
        "    ngram_transformers.append(ngram)\n",
        "\n",
        "  return tokenizer, ngram_transformers\n",
        "\n",
        "# Preprocessing Text\n",
        "\n",
        "df_clean = df_docs.withColumn(\n",
        "    \"text_clean\",\n",
        "    lower(regexp_replace(col(\"text\"), r\"[^a-zA-Z\\s]\", \"\"))\n",
        ")\n",
        "\n",
        "# Apply Tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"text_clean\", outputCol=\"words\")\n",
        "df_tokenized = tokenizer.transform(df_clean)\n",
        "\n",
        "# Applying n-grams transforemers for n = 2,3,4\n",
        "ngram_2 = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams_2\")\n",
        "ngram_3 = NGram(n=3, inputCol=\"words\", outputCol=\"ngrams_3\")\n",
        "ngram_4 = NGram(n=4, inputCol=\"words\", outputCol=\"ngrams_4\")\n",
        "\n",
        "df_shingles = ngram_2.transform(df_tokenized)\n",
        "df_shingles = ngram_3.transform(df_shingles)\n",
        "df_shingles = ngram_4.transform(df_shingles)\n",
        "\n",
        "print(\"Document with word shingles (n=2,3,4):\")\n",
        "df_shingles.select(\"id\", \"ngrams_2\", \"ngrams_3\").show(5, truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj8npgsOGTlZ",
        "outputId": "80f3c9d3-5e17-47bb-a3b1-cd5c02e40113"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document with word shingles (n=2,3,4):\n",
            "+---+--------------------------------------------------+--------------------------------------------------+\n",
            "| id|                                          ngrams_2|                                          ngrams_3|\n",
            "+---+--------------------------------------------------+--------------------------------------------------+\n",
            "|  0|[machine learning, learning is, is a, a subset,...|[machine learning is, learning is a, is a subse...|\n",
            "|  1|[artificial intelligence, intelligence and, and...|[artificial intelligence and, intelligence and ...|\n",
            "|  2|[deep learning, learning is, is a, a type, type...|[deep learning is, learning is a, is a type, a ...|\n",
            "|  3|[the weather, weather today, today is, is sunny...|[the weather today, weather today is, today is ...|\n",
            "|  4|[todays weather, weather forecast, forecast sho...|[todays weather forecast, weather forecast show...|\n",
            "+---+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: Combine all n-grams into a single sheet\n",
        "from pyspark.sql.functions import concat, array_union, array_distinct\n",
        "\n",
        "# Combine 2-grams, 3-grams and 4-grams into a single shingle set\n",
        "df_combined_shingles = df_shingles.withColumn(\n",
        "    \"all_shingles\",\n",
        "    array_distinct(\n",
        "        concat(col(\"ngrams_2\"), col(\"ngrams_3\"), col(\"ngrams_4\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Combined Shingless (2-4 grams):\")\n",
        "df_combined_shingles.select(\"id\", \"all_shingles\").show(3, truncate=80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF8TmQAXUCPO",
        "outputId": "1fd21d27-7150-4aaa-ba6e-5da09260690f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Shingless (2-4 grams):\n",
            "+---+--------------------------------------------------------------------------------+\n",
            "| id|                                                                    all_shingles|\n",
            "+---+--------------------------------------------------------------------------------+\n",
            "|  0|[machine learning, learning is, is a, a subset, subset of, of artificial, art...|\n",
            "|  1|[artificial intelligence, intelligence and, and machine, machine learning, le...|\n",
            "|  2|[deep learning, learning is, is a, a type, type of, of machine, machine learn...|\n",
            "+---+--------------------------------------------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}