{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NateMophi/SCC-454/blob/main/LAB2/SCC454_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvaH_Aif9u-V",
        "outputId": "30efbb35-9719-4ce4-db74-6d8124de0644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "dldqkvaN_Ajn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53b827d-49bd-4dbb-a81d-c9a7856e1545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jre-headless_11.0.29%2b7-1ubuntu1%7e22.04_amd64.deb  404  Not Found [IP: 91.189.92.22 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jdk-headless_11.0.29%2b7-1ubuntu1%7e22.04_amd64.deb  404  Not Found [IP: 91.189.92.22 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==4.0.0 -q\n",
        "\n",
        "# Java Installation\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "rZYa7FCMAQNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc8a5ded-b765-4289-955d-52a9ad7df7c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpark & Java installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Set Java environmenr variable\n",
        "import os\n",
        "os.environ[\"JAVA HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "print(\"PySpark & Java installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "qMwjYI6qAyUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af478700-a8e8-4c6f-c425-8d7348cd6820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Version: 4.0.0\n",
            "Spark App Name: SCC454-SparkIntro\n",
            "Master: local[*]\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "\n",
        "# Session Creation\n",
        "spark = SparkSession.builder.appName(\"SCC454-SparkIntro\").config(\"spark.driver.memory\", \"4g\")\\\n",
        "        .config(\"spark.ui.port\", \"4050\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "  # Underlying Context\n",
        "sc = spark.sparkContext\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Spark App Name: {spark.sparkContext.appName}\")\n",
        "print(f\"Master: {spark.sparkContext.master}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdzhTGEgG7Ac"
      },
      "source": [
        "# **Resilient Distributed Datasets (RDDs)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jrujuHElHEAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea0b7e9-94fa-4470-e9d1-6d6a5a9a7b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of paritions: 2\n",
            "First 5 elements : [1, 2, 3, 4, 5]\n"
          ]
        }
      ],
      "source": [
        "# RDD Creation\n",
        "\n",
        "# 1a) Parallelization from a Python Collection\n",
        "nums = [1,2,3,4,5,6,7,8,9,10]\n",
        "nums_rdd =sc.parallelize(nums)\n",
        "\n",
        "type(nums_rdd)\n",
        "print(f\"Number of paritions: {nums_rdd.getNumPartitions()}\")\n",
        "print(f\"First 5 elements : {nums_rdd.take(5)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BCrISMJPHD5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3991b786-07d6-481f-fa5d-32d858cd6192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions: 4\n",
            "\n",
            "Data in each partition: \n",
            "[[1, 2], [3, 4], [5, 6], [7, 8, 9, 10]]\n"
          ]
        }
      ],
      "source": [
        "# 1b) Parallelize with a specific number of partitions\n",
        "nums_rdd_4part = sc.parallelize(nums, 4)\n",
        "print(f\"Number of partitions: {nums_rdd_4part.getNumPartitions()}\")\n",
        "\n",
        "# View Partitions\n",
        "print(\"\\nData in each partition: \")\n",
        "print(nums_rdd_4part.glom().collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "anms-b20HFVc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379ef28a-2710-44c8-f99f-b40a2103013b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 5\n",
            "\n",
            "1st 3 lines:\n",
            " - Apache Spark is a unified analytics engine for large-scale data processing.\n",
            " - It provides high-level APIs in Java, Scala, Python and R.\n",
            " - Spark powers a stack of libraries including SQL and DataFrames.\n"
          ]
        }
      ],
      "source": [
        "# 2) Create RDD from text file\n",
        "sample_text = \"\"\"Apache Spark is a unified analytics engine for large-scale data processing.\n",
        "It provides high-level APIs in Java, Scala, Python and R.\n",
        "Spark powers a stack of libraries including SQL and DataFrames.\n",
        "It also includes MLlib for machine learning and GraphX for graph processing.\n",
        "Spark can run on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud.\"\"\"\n",
        "\n",
        "\n",
        "with open(\"spark_intro.txt\", \"w\") as f:\n",
        "  f.write(sample_text)\n",
        "\n",
        "# Load text file as RDD\n",
        "text_rdd = sc.textFile(\"spark_intro.txt\")\n",
        "print(f\"Number of lines: {text_rdd.count()}\")\n",
        "print(\"\\n1st 3 lines:\")\n",
        "for line in text_rdd.take(3):\n",
        "  print(f\" - {line}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Transformations and Actions\n",
        "\n",
        "RDD operations are divided into two categories:\n",
        "\n",
        "**Transformations**: Create a new RDD from an existing one. They are *lazy* - they don't execute until an action is called.\n",
        "\n",
        "| Transformation | Description |\n",
        "|----------------|-------------|\n",
        "| `map(func)` | Apply function to each element |\n",
        "| `filter(func)` | Keep elements where function returns true |\n",
        "| `flatMap(func)` | Map then flatten the results |\n",
        "| `distinct()` | Remove duplicates |\n",
        "| `reduceByKey(func)` | Aggregate values by key |\n",
        "| `groupByKey()` | Group values by key |\n",
        "| `sortBy(func)` | Sort RDD elements |\n",
        "\n",
        "**Actions**: Return a value to the driver program. They *trigger* the execution of transformations.\n",
        "\n",
        "| Action | Description |\n",
        "|--------|-------------|\n",
        "| `collect()` | Return all elements as a list |\n",
        "| `count()` | Return the number of elements |\n",
        "| `first()` | Return the first element |\n",
        "| `take(n)` | Return first n elements |\n",
        "| `reduce(func)` | Aggregate elements using a function |\n",
        "| `saveAsTextFile(path)` | Write elements to a text file |"
      ],
      "metadata": {
        "id": "nM9R03mN5t21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = sc.parallelize(range(1,11))\n",
        "\n",
        "# map: Apply function to each element\n",
        "squared = numbers.map(lambda x: x**2)\n",
        "print(f\"Original: {numbers.collect()} \")\n",
        "print(f\"Squared: {squared.collect()} \")\n",
        "\n",
        "# filter: Keep Only elements that match a condition\n",
        "evens = numbers.filter(lambda x: x%2==0)\n",
        "print(f\"Evens: {evens.collect()} \")\n",
        "\n",
        "# Chaining transformation\n",
        "even_squares = numbers.filter(lambda x: x%2==0).map(lambda x: x**2)\n",
        "print(f\"Even squares: {even_squares}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn7Nu2MnRI3Z",
        "outputId": "d7afcce3-b521-4b08-a5c8-c8d07d8d007d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
            "Squared: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] \n",
            "Evens: [2, 4, 6, 8, 10] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flatMap: map then Flatten\n",
        "sentences = sc.parallelize([\"Hello World\", \"Apache Spark\", \"Big Data\"])\n",
        "\n",
        "# map vs flatmap\n",
        "words_map = sentences.map(lambda s: s.split())\n",
        "words_flatmap = sentences.flatMap(lambda s: s.split())\n",
        "\n",
        "print(f\"Using map:     {words_map.collect()}\")\n",
        "print(f\"Using flatMap: {words_flatmap.collect()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMdeYyb96Leh",
        "outputId": "aa441fc8-c823-4d48-f726-30ec606d2d40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using map:     [['Hello', 'World'], ['Apache', 'Spark'], ['Big', 'Data']]\n",
            "Using flatMap: ['Hello', 'World', 'Apache', 'Spark', 'Big', 'Data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "id": "YxeWIM6NRI0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1940511e-da2c-4d4a-c254-d8f134ebefc9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[13] at readRDDFromFile at PythonRDD.scala:297"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Action Examples\n",
        "\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "print(f\"collect(): {numbers.collect()}\")\n",
        "print(f\"count():   {numbers.count()}\")\n",
        "print(f\"first():   {numbers.first()}\")\n",
        "print(f\"take(3):   {numbers.take(3)}\")\n",
        "print(f\"sum():     {numbers.sum()}\")\n",
        "print(f\"mean():    {numbers.mean()}\")\n",
        "print(f\"max():     {numbers.max()}\")\n",
        "print(f\"min():     {numbers.min()}\")\n",
        "\n",
        "# reduce: Aggregate elements\n",
        "total = numbers.reduce(lambda a, b: a + b)\n",
        "print(f\"reduce(+): {total}\")"
      ],
      "metadata": {
        "id": "n9cnJufeRIrn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff51351-6ff2-45ea-dd55-00b8fdbd7eb6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collect(): [1, 2, 3, 4, 5]\n",
            "count():   5\n",
            "first():   1\n",
            "take(3):   [1, 2, 3]\n",
            "sum():     15\n",
            "mean():    3.0\n",
            "max():     5\n",
            "min():     1\n",
            "reduce(+): 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Lazy Evaluation\n",
        "\n",
        "Spark transformations are *lazy* - they don't execute immediately. Instead, Spark builds up a *lineage graph* (DAG - Directed Acyclic Graph) of transformations. The actual computation only happens when an action is called.\n",
        "\n",
        "**Benefits of Lazy Evaluation:**\n",
        "- Allows Spark to optimize the execution plan\n",
        "- Reduces unnecessary computations\n",
        "- Enables fault tolerance through lineage"
      ],
      "metadata": {
        "id": "OQfEfBJz7-sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrating Lazy Evaluation\n",
        "import time\n",
        "\n",
        "# Create an RDD and apply transformations\n",
        "print(\"Creating RDD and transformations...\")\n",
        "start = time.time()\n",
        "\n",
        "large_rdd = sc.parallelize(range(1000000))\n",
        "transformed = large_rdd.map(lambda x: x * 2).filter(lambda x: x % 4 == 0)\n",
        "\n",
        "print(f\"Time to define transformations: {time.time() - start:.4f} seconds\")\n",
        "print(f\"Transformations are defined but NOT executed yet!\")\n",
        "print(f\"Type of 'transformed': {type(transformed)}\")\n",
        "\n",
        "# Now trigger execution with an action\n",
        "print(\"\\nCalling count() action...\")\n",
        "start = time.time()\n",
        "result = transformed.count()\n",
        "print(f\"Time to execute: {time.time() - start:.4f} seconds\")\n",
        "print(f\"Count: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejYYSdDN6Q8v",
        "outputId": "71b21795-1b5e-4ee2-cf27-bc5d08e502f4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating RDD and transformations...\n",
            "Time to define transformations: 0.0061 seconds\n",
            "Transformations are defined but NOT executed yet!\n",
            "Type of 'transformed': <class 'pyspark.core.rdd.PipelinedRDD'>\n",
            "\n",
            "Calling count() action...\n",
            "Time to execute: 1.1921 seconds\n",
            "Count: 500000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Word Count Example - The \"Hello World\" of Big Data\n",
        "\n",
        "Word Count is the classic MapReduce example. Let's implement it using Spark RDDs."
      ],
      "metadata": {
        "id": "KshqgtAG-jFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample text\n",
        "text = \"\"\"Spark is fast and general purpose cluster computing system\n",
        "Spark provides high level APIs in Java Scala Python and R\n",
        "Spark supports general computation graphs for data analysis\n",
        "Spark has rich set of higher level tools including Spark SQL\n",
        "Spark SQL provides support for structured data processing\"\"\"\n",
        "\n",
        "# Save to file\n",
        "with open(\"wordcount_input.txt\", \"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "8rlitK-W6Q1k"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Count using RDDs - Step by Step\n",
        "lines = sc.textFile(\"wordcount_input.txt\")\n",
        "\n",
        "# Step 1: Split each line into words\n",
        "words = lines.flatMap(lambda line: line.lower().split())\n",
        "print(\"Step 1 - Words:\")\n",
        "print(words.take(10))\n",
        "\n",
        "# Step 2: Map each word to a (word, 1) pair\n",
        "word_pairs = words.map(lambda word: (word, 1))\n",
        "print(\"\\nStep 2 - Word pairs:\")\n",
        "print(word_pairs.take(10))\n",
        "\n",
        "# Step 3: Reduce by key - sum up counts for each word\n",
        "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
        "print(\"\\nStep 3 - Word counts:\")\n",
        "print(word_counts.take(10))\n",
        "\n",
        "# Step 4: Sort by count (descending)\n",
        "sorted_counts = word_counts.sortBy(lambda x: -x[1])\n",
        "print(\"\\nTop 10 words:\")\n",
        "for word, count in sorted_counts.take(10):\n",
        "    print(f\"  {word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0DOMbXh6QzM",
        "outputId": "5520cdaf-2a94-445e-8fe7-3ca5cf8a89f2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1 - Words:\n",
            "['spark', 'is', 'fast', 'and', 'general', 'purpose', 'cluster', 'computing', 'system', 'spark']\n",
            "\n",
            "Step 2 - Word pairs:\n",
            "[('spark', 1), ('is', 1), ('fast', 1), ('and', 1), ('general', 1), ('purpose', 1), ('cluster', 1), ('computing', 1), ('system', 1), ('spark', 1)]\n",
            "\n",
            "Step 3 - Word counts:\n",
            "[('fast', 1), ('and', 2), ('general', 2), ('computing', 1), ('high', 1), ('level', 2), ('java', 1), ('python', 1), ('supports', 1), ('for', 2)]\n",
            "\n",
            "Top 10 words:\n",
            "  spark: 6\n",
            "  and: 2\n",
            "  general: 2\n",
            "  level: 2\n",
            "  for: 2\n",
            "  provides: 2\n",
            "  data: 2\n",
            "  sql: 2\n",
            "  fast: 1\n",
            "  computing: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Count - Concise Version (one-liner)\n",
        "\n",
        "word_counts_concise = sc.textFile(\"wordcount_input.txt\") \\\n",
        "    .flatMap(lambda line: line.lower().split()) \\\n",
        "    .map(lambda word: (word, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b) \\\n",
        "    .sortBy(lambda x: -x[1])\n",
        "\n",
        "print(\"Word counts (concise version):\")\n",
        "for word, count in word_counts_concise.take(10):\n",
        "    print(f\"  {word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv9Goe7P6Qw_",
        "outputId": "1d09e67c-2bf9-4658-a398-ac3da93c52a5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word counts (concise version):\n",
            "  spark: 6\n",
            "  and: 2\n",
            "  general: 2\n",
            "  level: 2\n",
            "  for: 2\n",
            "  provides: 2\n",
            "  data: 2\n",
            "  sql: 2\n",
            "  fast: 1\n",
            "  computing: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 3: Spark DataFrames\n",
        "---\n",
        "\n",
        "## 3.1 Introduction to DataFrames\n",
        "\n",
        "A DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database or a DataFrame in pandas. DataFrames are built on top of RDDs but provide:\n",
        "\n",
        "- **Schema**: Named columns with data types\n",
        "- **Optimized Execution**: Catalyst optimizer for query optimization\n",
        "- **Familiar API**: SQL-like operations\n",
        "\n",
        "**DataFrames vs RDDs:**\n",
        "\n",
        "| Feature | RDD | DataFrame |\n",
        "|---------|-----|----------|\n",
        "| Schema | No schema | Named columns with types |\n",
        "| Optimization | No automatic optimization | Catalyst optimizer |\n",
        "| Ease of use | Low-level API | High-level, SQL-like API |\n",
        "| Performance | Good | Better (optimized) |\n",
        "| Data types | Any Python object | Structured data types |"
      ],
      "metadata": {
        "id": "DVA5BS72HLfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Creating DataFrames\n",
        "\n",
        "There are multiple ways to create DataFrames in Spark."
      ],
      "metadata": {
        "id": "dBRIoUUrPKmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Method 1: From a list of tuples\n",
        "data = [\n",
        "    (\"Alice\", 28, \"Data Scientist\", 75000),\n",
        "    (\"Bob\", 35, \"Software Engineer\", 85000),\n",
        "    (\"Charlie\", 32, \"Data Analyst\", 65000),\n",
        "    (\"Diana\", 29, \"ML Engineer\", 90000),\n",
        "    (\"Eve\", 41, \"Data Scientist\", 95000)\n",
        "]\n",
        "columns = [\"name\", \"age\", \"role\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqstD8ov6Quq",
        "outputId": "5c53efd4-3899-4a7f-f848-2970c56773c4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-----------------+------+\n",
            "|   name|age|             role|salary|\n",
            "+-------+---+-----------------+------+\n",
            "|  Alice| 28|   Data Scientist| 75000|\n",
            "|    Bob| 35|Software Engineer| 85000|\n",
            "|Charlie| 32|     Data Analyst| 65000|\n",
            "|  Diana| 29|      ML Engineer| 90000|\n",
            "|    Eve| 41|   Data Scientist| 95000|\n",
            "+-------+---+-----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: With explicit schema\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"role\", StringType(), True),\n",
        "    StructField(\"salary\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "df_with_schema = spark.createDataFrame(data, schema)\n",
        "df_with_schema.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYaTIAZv6QsE",
        "outputId": "978dce10-eaa9-4a46-b515-63afd48b2fcc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- role: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bFmsgxNc6Qn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a_3dtrIyqFHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XJxoTJH1qFEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ASJLt_zXqFB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xtIRe6LvqE_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a0AfwMMQqE8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M1mau5epqE6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oC-6M_8VqE3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLI39Gt6qE1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DwTxYWGHqEyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nGlRAHxOqEwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mZYbVEA1qEtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WNGjV7TvqErF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}