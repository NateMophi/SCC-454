{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NateMophi/SCC-454/blob/main/SCC454_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dldqkvaN_Ajn",
        "outputId": "fc9d9664-2a1e-4cfb-afb2-0b714db9e0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==4.0.0 -q\n",
        "\n",
        "# Java Installation\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rZYa7FCMAQNo",
        "outputId": "826cdc1c-eccb-4cd0-f349-44fe88543445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PySpark & Java installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Set Java environmenr variable\n",
        "import os\n",
        "os.environ[\"JAVA HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "print(\"PySpark & Java installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qMwjYI6qAyUN",
        "outputId": "ecdb2986-8e0f-4a98-bf8e-0a6c084c4ee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Version: 4.0.0\n",
            "Spark App Name: SCC454-SparkIntro\n",
            "Master: local[*]\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "\n",
        "# Session Creation\n",
        "spark = SparkSession.builder.appName(\"SCC454-SparkIntro\").config(\"spark.driver.memory\", \"4g\")\\\n",
        "        .config(\"spark.ui.port\", \"4050\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "  # Underlying Context\n",
        "sc = spark.sparkContext\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Spark App Name: {spark.sparkContext.appName}\")\n",
        "print(f\"Master: {spark.sparkContext.master}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdzhTGEgG7Ac"
      },
      "source": [
        "# **Resilient Distributed Datasets (RDDs)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrujuHElHEAd",
        "outputId": "08d04f93-f350-4380-d2f4-f42c1eaacdf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of paritions: 2\n",
            "First 5 elements : [1, 2, 3, 4, 5]\n"
          ]
        }
      ],
      "source": [
        "# RDD Creation\n",
        "\n",
        "# 1a) Parallelization from a Python Collection\n",
        "nums = [1,2,3,4,5,6,7,8,9,10]\n",
        "nums_rdd =sc.parallelize(nums)\n",
        "\n",
        "type(nums_rdd)\n",
        "print(f\"Number of paritions: {nums_rdd.getNumPartitions()}\")\n",
        "print(f\"First 5 elements : {nums_rdd.take(5)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCrISMJPHD5x",
        "outputId": "9527c355-43a1-4ddb-a16a-df64cfb6e9fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of partitions: 4\n",
            "\n",
            "Data in each partition: \n",
            "[[1, 2], [3, 4], [5, 6], [7, 8, 9, 10]]\n"
          ]
        }
      ],
      "source": [
        "# 1b) Parallelize with a specific number of partitions\n",
        "nums_rdd_4part = sc.parallelize(nums, 4)\n",
        "print(f\"Number of partitions: {nums_rdd_4part.getNumPartitions()}\")\n",
        "\n",
        "# View Partitions\n",
        "print(\"\\nData in each partition: \")\n",
        "print(nums_rdd_4part.glom().collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anms-b20HFVc",
        "outputId": "80568ccb-e542-484b-e11d-58dba08e93c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of lines: 5\n",
            "\n",
            "1st 3 lines:\n",
            " - Apache Spark is a unified analytics engine for large-scale data processing.\n",
            " - It provides high-level APIs in Java, Scala, Python and R.\n",
            " - Spark powers a stack of libraries including SQL and DataFrames.\n"
          ]
        }
      ],
      "source": [
        "# 2) Create RDD from text file\n",
        "sample_text = \"\"\"Apache Spark is a unified analytics engine for large-scale data processing.\n",
        "It provides high-level APIs in Java, Scala, Python and R.\n",
        "Spark powers a stack of libraries including SQL and DataFrames.\n",
        "It also includes MLlib for machine learning and GraphX for graph processing.\n",
        "Spark can run on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud.\"\"\"\n",
        "\n",
        "\n",
        "with open(\"spark_intro.txt\", \"w\") as f:\n",
        "  f.write(sample_text)\n",
        "\n",
        "# Load text file as RDD\n",
        "text_rdd = sc.textFile(\"spark_intro.txt\")\n",
        "print(f\"Number of lines: {text_rdd.count()}\")\n",
        "print(\"\\n1st 3 lines:\")\n",
        "for line in text_rdd.take(3):\n",
        "  print(f\" - {line}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
