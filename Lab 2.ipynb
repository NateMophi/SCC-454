{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Lancaster University](https://www.lancaster.ac.uk/media/lancaster-university/content-assets/images/fst/logos/SCC-Logo.svg)\n",
        "\n",
        "# SCC.454: Large Scale Platforms for AI and Data Analysis\n",
        "## Lab 2: Introduction to Apache Spark\n",
        "\n",
        "**Duration:** 2 hours\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand what Apache Spark is and why it's used for big data processing\n",
        "- Learn to set up and configure Spark in Google Colab\n",
        "- Master the fundamentals of RDDs (Resilient Distributed Datasets)\n",
        "- Work with Spark DataFrames and the DataFrame API\n",
        "- Execute SQL queries using Spark SQL\n",
        "- Apply Spark to real-world datasets"
      ],
      "metadata": {
        "id": "intro_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. **Part 1: Introduction to Apache Spark** (20 minutes)\n",
        "   - What is Apache Spark?\n",
        "   - Spark Architecture Overview\n",
        "   - Setting up Spark in Google Colab\n",
        "   - Creating Your First SparkSession\n",
        "\n",
        "2. **Part 2: RDDs - Resilient Distributed Datasets** (30 minutes)\n",
        "   - Understanding RDDs\n",
        "   - Creating RDDs\n",
        "   - Transformations and Actions\n",
        "   - Lazy Evaluation\n",
        "   - Word Count Example\n",
        "\n",
        "3. **Part 3: Spark DataFrames** (40 minutes)\n",
        "   - Introduction to DataFrames\n",
        "   - Loading Data from Various Sources\n",
        "   - DataFrame Operations\n",
        "   - Working with Real-World Data: Global Temperature Dataset\n",
        "   - Data Aggregation and Grouping\n",
        "\n",
        "4. **Part 4: Spark SQL** (20 minutes)\n",
        "   - Introduction to Spark SQL\n",
        "   - Creating Temporary Views\n",
        "   - Writing SQL Queries\n",
        "   - Combining DataFrame API and SQL\n",
        "\n",
        "5. **Part 5: Practical Exercises** (10 minutes)\n",
        "   - Exercise 1: Movie Ratings Analysis\n",
        "   - Exercise 2: E-commerce Sales Analysis\n",
        "   - Challenge Exercise"
      ],
      "metadata": {
        "id": "toc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 1: Introduction to Apache Spark\n",
        "---\n",
        "\n",
        "## 1.1 What is Apache Spark?\n",
        "\n",
        "Apache Spark is a unified analytics engine for large-scale data processing. Originally developed at UC Berkeley's AMPLab in 2009, Spark was designed to overcome the limitations of the Hadoop MapReduce model while maintaining its benefits.\n",
        "\n",
        "**Key Features of Spark:**\n",
        "\n",
        "- **Speed**: Spark can be up to 100x faster than Hadoop MapReduce for certain applications, thanks to in-memory computing\n",
        "- **Ease of Use**: Provides high-level APIs in Python, Java, Scala, and R\n",
        "- **Generality**: Supports SQL queries, streaming data, machine learning, and graph processing\n",
        "- **Runs Everywhere**: Can run on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud\n",
        "\n",
        "**When to use Spark:**\n",
        "- Processing large datasets (gigabytes to petabytes)\n",
        "- Iterative algorithms (machine learning)\n",
        "- Interactive data analysis\n",
        "- Stream processing\n",
        "- ETL (Extract, Transform, Load) operations"
      ],
      "metadata": {
        "id": "part1_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Spark Architecture Overview\n",
        "\n",
        "Spark follows a master-worker architecture:\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────┐\n",
        "│                   Driver Program                     │\n",
        "│  ┌─────────────────────────────────────────────┐    │\n",
        "│  │              SparkContext                    │    │\n",
        "│  └─────────────────────────────────────────────┘    │\n",
        "└───────────────────────┬─────────────────────────────┘\n",
        "                        │\n",
        "        ┌───────────────┼───────────────┐\n",
        "        │               │               │\n",
        "        ▼               ▼               ▼\n",
        "┌───────────────┐ ┌───────────────┐ ┌───────────────┐\n",
        "│   Worker 1    │ │   Worker 2    │ │   Worker N    │\n",
        "│  ┌─────────┐  │ │  ┌─────────┐  │ │  ┌─────────┐  │\n",
        "│  │Executor │  │ │  │Executor │  │ │  │Executor │  │\n",
        "│  │ Tasks   │  │ │  │ Tasks   │  │ │  │ Tasks   │  │\n",
        "│  └─────────┘  │ │  └─────────┘  │ │  └─────────┘  │\n",
        "└───────────────┘ └───────────────┘ └───────────────┘\n",
        "```\n",
        "\n",
        "**Components:**\n",
        "- **Driver Program**: The main program that creates the SparkContext and coordinates the execution\n",
        "- **SparkContext**: The entry point to Spark functionality, connects to the cluster\n",
        "- **Cluster Manager**: Allocates resources across applications (YARN, Mesos, Kubernetes, or standalone)\n",
        "- **Executors**: Processes that run on worker nodes, execute tasks and store data"
      ],
      "metadata": {
        "id": "architecture"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Setting up Spark in Google Colab\n",
        "\n",
        "Google Colab doesn't have Spark pre-installed, so we need to set it up manually. Run the cell below to install PySpark and configure the Java environment.\n",
        "\n",
        "If you are running it on a different system make sure you give the correct JAVA_HOME\n",
        "\n",
        "Do not worry about the pip error you see here"
      ],
      "metadata": {
        "id": "setup_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PySpark\n",
        "!pip install pyspark==3.5.0 -q\n",
        "\n",
        "# Install Java (Spark requires Java)\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Set Java environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "print(\"PySpark and Java installed successfully!\")"
      ],
      "metadata": {
        "id": "install_spark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Creating Your First SparkSession\n",
        "\n",
        "The `SparkSession` is the entry point to Spark functionality in Spark 2.0+. It provides a unified interface to work with DataFrames, SQL, and streaming.\n",
        "\n",
        "In earlier versions of Spark (1.x), you needed separate contexts:\n",
        "- `SparkContext` for RDDs\n",
        "- `SQLContext` for DataFrames and SQL\n",
        "- `HiveContext` for Hive integration\n",
        "\n",
        "`SparkSession` combines all of these into a single, unified entry point."
      ],
      "metadata": {
        "id": "sparksession_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SCC454-SparkIntro\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Get the underlying SparkContext\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
        "print(f\"Master: {spark.sparkContext.master}\")"
      ],
      "metadata": {
        "id": "create_session"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can display the SparkSession object to see its configuration:"
      ],
      "metadata": {
        "id": "display_session_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "display_session"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding Local Mode:**\n",
        "\n",
        "When you see `Master: local[*]`, it means Spark is running in local mode using all available CPU cores. This is perfect for learning and development. In production, you would connect to a cluster manager like YARN or Kubernetes.\n",
        "\n",
        "The `local[N]` notation means:\n",
        "- `local`: Run Spark locally with one thread\n",
        "- `local[2]`: Run Spark locally with 2 worker threads\n",
        "- `local[*]`: Run Spark locally with as many threads as CPU cores"
      ],
      "metadata": {
        "id": "local_mode_explain"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 2: RDDs - Resilient Distributed Datasets\n",
        "---\n",
        "\n",
        "## 2.1 Understanding RDDs\n",
        "\n",
        "RDD (Resilient Distributed Dataset) is the fundamental data structure of Spark. It's an immutable, distributed collection of objects that can be processed in parallel.\n",
        "\n",
        "**Key Properties of RDDs:**\n",
        "\n",
        "- **Resilient**: Can recover from node failures through lineage\n",
        "- **Distributed**: Data is distributed across multiple nodes in the cluster\n",
        "- **Dataset**: Collection of partitioned data with primitive values or custom objects\n",
        "\n",
        "**RDD Characteristics:**\n",
        "- Immutable: Once created, the data in an RDD cannot be changed\n",
        "- Lazily Evaluated: Transformations are not executed until an action is called\n",
        "- Cacheable: Can be cached in memory for faster access"
      ],
      "metadata": {
        "id": "rdd_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Creating RDDs\n",
        "\n",
        "There are two main ways to create RDDs:\n",
        "1. **Parallelizing an existing collection** in your driver program\n",
        "2. **Loading data from external storage** (files, databases, etc.)"
      ],
      "metadata": {
        "id": "creating_rdds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: Parallelize a Python collection\n",
        "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "numbers_rdd = sc.parallelize(numbers)\n",
        "\n",
        "print(f\"Type: {type(numbers_rdd)}\")\n",
        "print(f\"Number of partitions: {numbers_rdd.getNumPartitions()}\")\n",
        "print(f\"First 5 elements: {numbers_rdd.take(5)}\")"
      ],
      "metadata": {
        "id": "create_rdd_parallelize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1b: Parallelize with specified number of partitions\n",
        "numbers_rdd_4part = sc.parallelize(numbers, 4)\n",
        "print(f\"Number of partitions: {numbers_rdd_4part.getNumPartitions()}\")\n",
        "\n",
        "# View the partitions\n",
        "print(\"\\nData in each partition:\")\n",
        "print(numbers_rdd_4part.glom().collect())"
      ],
      "metadata": {
        "id": "rdd_partitions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: Create RDD from a text file\n",
        "# First, let's create a sample text file\n",
        "\n",
        "sample_text = \"\"\"Apache Spark is a unified analytics engine for large-scale data processing.\n",
        "It provides high-level APIs in Java, Scala, Python and R.\n",
        "Spark powers a stack of libraries including SQL and DataFrames.\n",
        "It also includes MLlib for machine learning and GraphX for graph processing.\n",
        "Spark can run on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud.\"\"\"\n",
        "\n",
        "with open(\"spark_intro.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "# Load the text file as an RDD\n",
        "text_rdd = sc.textFile(\"spark_intro.txt\")\n",
        "print(f\"Number of lines: {text_rdd.count()}\")\n",
        "print(\"\\nFirst 3 lines:\")\n",
        "for line in text_rdd.take(3):\n",
        "    print(f\"  - {line}\")"
      ],
      "metadata": {
        "id": "create_rdd_file"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Transformations and Actions\n",
        "\n",
        "RDD operations are divided into two categories:\n",
        "\n",
        "**Transformations**: Create a new RDD from an existing one. They are *lazy* - they don't execute until an action is called.\n",
        "\n",
        "| Transformation | Description |\n",
        "|----------------|-------------|\n",
        "| `map(func)` | Apply function to each element |\n",
        "| `filter(func)` | Keep elements where function returns true |\n",
        "| `flatMap(func)` | Map then flatten the results |\n",
        "| `distinct()` | Remove duplicates |\n",
        "| `reduceByKey(func)` | Aggregate values by key |\n",
        "| `groupByKey()` | Group values by key |\n",
        "| `sortBy(func)` | Sort RDD elements |\n",
        "\n",
        "**Actions**: Return a value to the driver program. They *trigger* the execution of transformations.\n",
        "\n",
        "| Action | Description |\n",
        "|--------|-------------|\n",
        "| `collect()` | Return all elements as a list |\n",
        "| `count()` | Return the number of elements |\n",
        "| `first()` | Return the first element |\n",
        "| `take(n)` | Return first n elements |\n",
        "| `reduce(func)` | Aggregate elements using a function |\n",
        "| `saveAsTextFile(path)` | Write elements to a text file |"
      ],
      "metadata": {
        "id": "transformations_actions"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation Examples\n",
        "\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "# map: Apply a function to each element\n",
        "squared = numbers.map(lambda x: x ** 2)\n",
        "print(f\"Original: {numbers.collect()}\")\n",
        "print(f\"Squared:  {squared.collect()}\")\n",
        "\n",
        "# filter: Keep only elements that match a condition\n",
        "evens = numbers.filter(lambda x: x % 2 == 0)\n",
        "print(f\"Evens:    {evens.collect()}\")\n",
        "\n",
        "# Chaining transformations\n",
        "even_squares = numbers.filter(lambda x: x % 2 == 0).map(lambda x: x ** 2)\n",
        "print(f\"Even squares: {even_squares.collect()}\")"
      ],
      "metadata": {
        "id": "transformation_examples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flatMap: Map then flatten\n",
        "sentences = sc.parallelize([\"Hello World\", \"Apache Spark\", \"Big Data\"])\n",
        "\n",
        "# map vs flatMap\n",
        "words_map = sentences.map(lambda s: s.split())\n",
        "words_flatmap = sentences.flatMap(lambda s: s.split())\n",
        "\n",
        "print(f\"Using map:     {words_map.collect()}\")\n",
        "print(f\"Using flatMap: {words_flatmap.collect()}\")"
      ],
      "metadata": {
        "id": "flatmap_example"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Action Examples\n",
        "\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "print(f\"collect(): {numbers.collect()}\")\n",
        "print(f\"count():   {numbers.count()}\")\n",
        "print(f\"first():   {numbers.first()}\")\n",
        "print(f\"take(3):   {numbers.take(3)}\")\n",
        "print(f\"sum():     {numbers.sum()}\")\n",
        "print(f\"mean():    {numbers.mean()}\")\n",
        "print(f\"max():     {numbers.max()}\")\n",
        "print(f\"min():     {numbers.min()}\")\n",
        "\n",
        "# reduce: Aggregate elements\n",
        "total = numbers.reduce(lambda a, b: a + b)\n",
        "print(f\"reduce(+): {total}\")"
      ],
      "metadata": {
        "id": "action_examples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Lazy Evaluation\n",
        "\n",
        "Spark transformations are *lazy* - they don't execute immediately. Instead, Spark builds up a *lineage graph* (DAG - Directed Acyclic Graph) of transformations. The actual computation only happens when an action is called.\n",
        "\n",
        "**Benefits of Lazy Evaluation:**\n",
        "- Allows Spark to optimize the execution plan\n",
        "- Reduces unnecessary computations\n",
        "- Enables fault tolerance through lineage"
      ],
      "metadata": {
        "id": "lazy_eval"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrating Lazy Evaluation\n",
        "import time\n",
        "\n",
        "# Create an RDD and apply transformations\n",
        "print(\"Creating RDD and transformations...\")\n",
        "start = time.time()\n",
        "\n",
        "large_rdd = sc.parallelize(range(1000000))\n",
        "transformed = large_rdd.map(lambda x: x * 2).filter(lambda x: x % 4 == 0)\n",
        "\n",
        "print(f\"Time to define transformations: {time.time() - start:.4f} seconds\")\n",
        "print(f\"Transformations are defined but NOT executed yet!\")\n",
        "print(f\"Type of 'transformed': {type(transformed)}\")\n",
        "\n",
        "# Now trigger execution with an action\n",
        "print(\"\\nCalling count() action...\")\n",
        "start = time.time()\n",
        "result = transformed.count()\n",
        "print(f\"Time to execute: {time.time() - start:.4f} seconds\")\n",
        "print(f\"Count: {result}\")"
      ],
      "metadata": {
        "id": "lazy_demo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Word Count Example - The \"Hello World\" of Big Data\n",
        "\n",
        "Word Count is the classic MapReduce example. Let's implement it using Spark RDDs."
      ],
      "metadata": {
        "id": "wordcount_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample text\n",
        "text = \"\"\"Spark is fast and general purpose cluster computing system\n",
        "Spark provides high level APIs in Java Scala Python and R\n",
        "Spark supports general computation graphs for data analysis\n",
        "Spark has rich set of higher level tools including Spark SQL\n",
        "Spark SQL provides support for structured data processing\"\"\"\n",
        "\n",
        "# Save to file\n",
        "with open(\"wordcount_input.txt\", \"w\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "# Word Count using RDDs - Step by Step\n",
        "lines = sc.textFile(\"wordcount_input.txt\")\n",
        "\n",
        "# Step 1: Split each line into words\n",
        "words = lines.flatMap(lambda line: line.lower().split())\n",
        "print(\"Step 1 - Words:\")\n",
        "print(words.take(10))\n",
        "\n",
        "# Step 2: Map each word to a (word, 1) pair\n",
        "word_pairs = words.map(lambda word: (word, 1))\n",
        "print(\"\\nStep 2 - Word pairs:\")\n",
        "print(word_pairs.take(10))\n",
        "\n",
        "# Step 3: Reduce by key - sum up counts for each word\n",
        "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
        "print(\"\\nStep 3 - Word counts:\")\n",
        "print(word_counts.take(10))\n",
        "\n",
        "# Step 4: Sort by count (descending)\n",
        "sorted_counts = word_counts.sortBy(lambda x: -x[1])\n",
        "print(\"\\nTop 10 words:\")\n",
        "for word, count in sorted_counts.take(10):\n",
        "    print(f\"  {word}: {count}\")"
      ],
      "metadata": {
        "id": "wordcount_detailed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Count - Concise Version (one-liner)\n",
        "\n",
        "word_counts_concise = sc.textFile(\"wordcount_input.txt\") \\\n",
        "    .flatMap(lambda line: line.lower().split()) \\\n",
        "    .map(lambda word: (word, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b) \\\n",
        "    .sortBy(lambda x: -x[1])\n",
        "\n",
        "print(\"Word counts (concise version):\")\n",
        "for word, count in word_counts_concise.take(10):\n",
        "    print(f\"  {word}: {count}\")"
      ],
      "metadata": {
        "id": "wordcount_concise"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 3: Spark DataFrames\n",
        "---\n",
        "\n",
        "## 3.1 Introduction to DataFrames\n",
        "\n",
        "A DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database or a DataFrame in pandas. DataFrames are built on top of RDDs but provide:\n",
        "\n",
        "- **Schema**: Named columns with data types\n",
        "- **Optimized Execution**: Catalyst optimizer for query optimization\n",
        "- **Familiar API**: SQL-like operations\n",
        "\n",
        "**DataFrames vs RDDs:**\n",
        "\n",
        "| Feature | RDD | DataFrame |\n",
        "|---------|-----|----------|\n",
        "| Schema | No schema | Named columns with types |\n",
        "| Optimization | No automatic optimization | Catalyst optimizer |\n",
        "| Ease of use | Low-level API | High-level, SQL-like API |\n",
        "| Performance | Good | Better (optimized) |\n",
        "| Data types | Any Python object | Structured data types |"
      ],
      "metadata": {
        "id": "dataframe_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Creating DataFrames\n",
        "\n",
        "There are multiple ways to create DataFrames in Spark."
      ],
      "metadata": {
        "id": "create_df_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Method 1: From a list of tuples\n",
        "data = [\n",
        "    (\"Alice\", 28, \"Data Scientist\", 75000),\n",
        "    (\"Bob\", 35, \"Software Engineer\", 85000),\n",
        "    (\"Charlie\", 32, \"Data Analyst\", 65000),\n",
        "    (\"Diana\", 29, \"ML Engineer\", 90000),\n",
        "    (\"Eve\", 41, \"Data Scientist\", 95000)\n",
        "]\n",
        "columns = [\"name\", \"age\", \"role\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "create_df_tuples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: With explicit schema\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"role\", StringType(), True),\n",
        "    StructField(\"salary\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "df_with_schema = spark.createDataFrame(data, schema)\n",
        "df_with_schema.printSchema()"
      ],
      "metadata": {
        "id": "create_df_schema"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 3: From a pandas DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "pandas_df = pd.DataFrame({\n",
        "    \"city\": [\"London\", \"Manchester\", \"Birmingham\", \"Leeds\", \"Glasgow\"],\n",
        "    \"population\": [8982000, 547627, 1141816, 793139, 626410],\n",
        "    \"country\": [\"England\", \"England\", \"England\", \"England\", \"Scotland\"]\n",
        "})\n",
        "\n",
        "spark_df = spark.createDataFrame(pandas_df)\n",
        "spark_df.show()"
      ],
      "metadata": {
        "id": "create_df_pandas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 4: From Row objects\n",
        "row_data = [\n",
        "    Row(product=\"Laptop\", price=999.99, quantity=50),\n",
        "    Row(product=\"Mouse\", price=29.99, quantity=200),\n",
        "    Row(product=\"Keyboard\", price=79.99, quantity=150)\n",
        "]\n",
        "\n",
        "df_rows = spark.createDataFrame(row_data)\n",
        "df_rows.show()"
      ],
      "metadata": {
        "id": "create_df_rows"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Loading Data from External Sources\n",
        "\n",
        "Spark can read data from various formats including CSV, JSON, Parquet, and more. Let's download a real-world dataset to work with."
      ],
      "metadata": {
        "id": "load_data_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Global Land Temperatures dataset from NASA GISS\n",
        "# This is the official GISTEMP v4 dataset (Global Land-Ocean Temperature Index)\n",
        "\n",
        "!wget -q https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv -O global_temp.csv\n",
        "\n",
        "# Preview the file\n",
        "!head -15 global_temp.csv"
      ],
      "metadata": {
        "id": "download_temp_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpler approach: use pandas to clean, then convert to Spark\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col, lit, explode, array, struct\n",
        "\n",
        "# Read with pandas, skip the first row\n",
        "pdf = pd.read_csv(\"global_temp.csv\", skiprows=1)\n",
        "print(\"Pandas columns:\", pdf.columns.tolist())\n",
        "print(pdf.head())\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "temp_df_raw = spark.createDataFrame(pdf)\n",
        "\n",
        "print(\"\\nSpark DataFrame:\")\n",
        "temp_df_raw.show(5)\n",
        "print(\"\\nSchema:\")\n",
        "temp_df_raw.printSchema()\n",
        "\n",
        "# Now transform to long format\n",
        "month_cols = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "\n",
        "temp_df = temp_df_raw.select(\n",
        "    col(\"Year\"),\n",
        "    explode(array([struct(lit(i+1).alias(\"Month\"), col(m).cast(\"double\").alias(\"Mean\"))\n",
        "                   for i, m in enumerate(month_cols)])).alias(\"data\")\n",
        ").select(\n",
        "    col(\"Year\").cast(\"int\").alias(\"Year\"),\n",
        "    col(\"data.Month\").alias(\"Month\"),\n",
        "    col(\"data.Mean\").alias(\"Mean\")\n",
        ").filter(col(\"Mean\").isNotNull())\n",
        "\n",
        "# Add a Source column for consistency\n",
        "temp_df = temp_df.withColumn(\"Source\", lit(\"GISTEMP\"))\n",
        "\n",
        "print(\"\\nTransformed data (long format):\")\n",
        "temp_df.show(10)\n",
        "\n",
        "print(\"\\nSchema:\")\n",
        "temp_df.printSchema()\n",
        "\n",
        "print(f\"\\nTotal records: {temp_df.count()}\")"
      ],
      "metadata": {
        "id": "load_csv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 DataFrame Operations\n",
        "\n",
        "DataFrames support a rich set of operations for data manipulation and analysis."
      ],
      "metadata": {
        "id": "df_operations_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, year, month, avg, max, min, count, round\n",
        "\n",
        "# Select columns\n",
        "print(\"Select specific columns:\")\n",
        "temp_df.select(\"Year\", \"Month\", \"Mean\").show(5)\n",
        "\n",
        "# Select with expressions - convert to Fahrenheit\n",
        "print(\"\\nSelect with expressions (add Fahrenheit):\")\n",
        "temp_df.select(\n",
        "    col(\"Year\"),\n",
        "    col(\"Month\"),\n",
        "    col(\"Mean\"),\n",
        "    round(col(\"Mean\") * 1.8 + 32, 2).alias(\"Mean_Fahrenheit\")\n",
        ").show(5)"
      ],
      "metadata": {
        "id": "df_select"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter rows\n",
        "print(\"Filter: Temperatures above 0.5°C anomaly:\")\n",
        "temp_df.filter(col(\"Mean\") > 0.5).show(5)\n",
        "\n",
        "# Multiple conditions\n",
        "print(\"\\nFilter: Year 2020 or later with mean > 0.8:\")\n",
        "temp_df.filter(\n",
        "    (col(\"Year\") >= 2020) & (col(\"Mean\") > 0.8)\n",
        ").show(10)"
      ],
      "metadata": {
        "id": "df_filter"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new columns with withColumn\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "# The data already has Year and Month columns, let's add derived columns\n",
        "temp_df_enhanced = temp_df \\\n",
        "    .withColumn(\"Mean_F\", round(col(\"Mean\") * 1.8 + 32, 2)) \\\n",
        "    .withColumn(\"Decade\", (col(\"Year\") / 10).cast(\"int\") * 10) \\\n",
        "    .withColumn(\"Season\",\n",
        "                when(col(\"Month\").isin(12, 1, 2), \"Winter\")\n",
        "                .when(col(\"Month\").isin(3, 4, 5), \"Spring\")\n",
        "                .when(col(\"Month\").isin(6, 7, 8), \"Summer\")\n",
        "                .otherwise(\"Autumn\"))\n",
        "\n",
        "temp_df_enhanced.show(10)"
      ],
      "metadata": {
        "id": "df_withcolumn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting\n",
        "print(\"Sorted by Mean (descending) - Warmest temperature anomalies:\")\n",
        "temp_df_enhanced.orderBy(col(\"Mean\").desc()).show(10)\n",
        "\n",
        "print(\"\\nSorted by Mean (ascending) - Coldest temperature anomalies:\")\n",
        "temp_df_enhanced.orderBy(col(\"Mean\").asc()).show(10)"
      ],
      "metadata": {
        "id": "df_sort"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Data Aggregation and Grouping\n",
        "\n",
        "Aggregation operations are essential for data analysis. Spark provides powerful groupBy and aggregation functions."
      ],
      "metadata": {
        "id": "aggregation_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic aggregations\n",
        "print(\"Overall statistics:\")\n",
        "temp_df_enhanced.agg(\n",
        "    count(\"*\").alias(\"total_records\"),\n",
        "    round(avg(\"Mean\"), 4).alias(\"avg_temp\"),\n",
        "    round(max(\"Mean\"), 4).alias(\"max_temp\"),\n",
        "    round(min(\"Mean\"), 4).alias(\"min_temp\")\n",
        ").show()"
      ],
      "metadata": {
        "id": "basic_aggregation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by Year and calculate average temperature\n",
        "yearly_avg = temp_df_enhanced \\\n",
        "    .groupBy(\"Year\") \\\n",
        "    .agg(\n",
        "        round(avg(\"Mean\"), 4).alias(\"Avg_Temp\"),\n",
        "        round(max(\"Mean\"), 4).alias(\"Max_Temp\"),\n",
        "        round(min(\"Mean\"), 4).alias(\"Min_Temp\")\n",
        "    ) \\\n",
        "    .orderBy(\"Year\")\n",
        "\n",
        "print(\"Yearly temperature statistics (GISTEMP):\")\n",
        "yearly_avg.show(20)"
      ],
      "metadata": {
        "id": "groupby_year"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare by decade\n",
        "decade_comparison = temp_df_enhanced \\\n",
        "    .groupBy(\"Decade\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"Record_Count\"),\n",
        "        round(avg(\"Mean\"), 4).alias(\"Avg_Temp\"),\n",
        "        round(max(\"Mean\"), 4).alias(\"Max_Temp\"),\n",
        "        round(min(\"Mean\"), 4).alias(\"Min_Temp\")\n",
        "    ) \\\n",
        "    .orderBy(\"Decade\")\n",
        "\n",
        "print(\"Temperature anomalies by Decade:\")\n",
        "decade_comparison.show()"
      ],
      "metadata": {
        "id": "groupby_source"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the trends using matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert to pandas for plotting\n",
        "yearly_pandas = yearly_avg.toPandas()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(yearly_pandas[\"Year\"], yearly_pandas[\"Avg_Temp\"], marker='o', markersize=3, linewidth=1)\n",
        "plt.xlabel(\"Year\", fontsize=12)\n",
        "plt.ylabel(\"Average Temperature Anomaly (°C)\", fontsize=12)\n",
        "plt.title(\"Global Temperature Anomaly by Year (NASA GISTEMP v4)\", fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='1951-1980 Baseline')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "visualization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Working with Missing Data"
      ],
      "metadata": {
        "id": "missing_data_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, isnan, isnull\n",
        "\n",
        "# Create a DataFrame with missing values\n",
        "data_with_nulls = [\n",
        "    (\"A\", 25, 50000.0),\n",
        "    (\"B\", None, 60000.0),\n",
        "    (\"C\", 35, None),\n",
        "    (\"D\", None, None),\n",
        "    (\"E\", 45, 80000.0)\n",
        "]\n",
        "\n",
        "df_nulls = spark.createDataFrame(data_with_nulls, [\"name\", \"age\", \"salary\"])\n",
        "print(\"DataFrame with null values:\")\n",
        "df_nulls.show()\n",
        "\n",
        "# Count nulls per column\n",
        "print(\"Null counts per column:\")\n",
        "df_nulls.select([\n",
        "    count(when(col(c).isNull(), c)).alias(c)\n",
        "    for c in df_nulls.columns\n",
        "]).show()"
      ],
      "metadata": {
        "id": "check_nulls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with any null values\n",
        "print(\"Drop rows with any null:\")\n",
        "df_nulls.na.drop().show()\n",
        "\n",
        "# Drop rows where specific columns are null\n",
        "print(\"Drop rows where age is null:\")\n",
        "df_nulls.na.drop(subset=[\"age\"]).show()\n",
        "\n",
        "# Fill null values\n",
        "print(\"Fill nulls with 0:\")\n",
        "df_nulls.na.fill(0).show()\n",
        "\n",
        "# Fill with different values per column\n",
        "print(\"Fill with specific values per column:\")\n",
        "df_nulls.na.fill({\"age\": 30, \"salary\": 50000.0}).show()"
      ],
      "metadata": {
        "id": "handle_nulls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 4: Spark SQL\n",
        "---\n",
        "\n",
        "## 4.1 Introduction to Spark SQL\n",
        "\n",
        "Spark SQL allows you to query structured data using SQL syntax. This is particularly useful if you're already familiar with SQL or need to integrate with SQL-based tools.\n",
        "\n",
        "**Advantages of Spark SQL:**\n",
        "- Familiar SQL syntax for data analysts\n",
        "- Automatic query optimization via Catalyst\n",
        "- Integration with Hive and other data sources\n",
        "- Mix SQL with DataFrame operations"
      ],
      "metadata": {
        "id": "sparksql_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Creating Temporary Views\n",
        "\n",
        "To use SQL queries, you first need to register your DataFrame as a temporary view."
      ],
      "metadata": {
        "id": "temp_views"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the temperature DataFrame as a temporary view\n",
        "temp_df_enhanced.createOrReplaceTempView(\"temperatures\")\n",
        "\n",
        "print(\"Temporary view 'temperatures' created successfully!\")\n",
        "print(\"\\nAvailable columns:\")\n",
        "print(temp_df_enhanced.columns)"
      ],
      "metadata": {
        "id": "create_view"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Writing SQL Queries\n",
        "\n",
        "Once you have a temporary view, you can run SQL queries using `spark.sql()`."
      ],
      "metadata": {
        "id": "sql_queries_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic SELECT query\n",
        "result = spark.sql(\"\"\"\n",
        "    SELECT Year, Month, Mean, Mean_F, Season\n",
        "    FROM temperatures\n",
        "    LIMIT 10\n",
        "\"\"\")\n",
        "result.show()"
      ],
      "metadata": {
        "id": "sql_select"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering with WHERE\n",
        "high_temps = spark.sql(\"\"\"\n",
        "    SELECT Year, Month, Mean, Season, Decade\n",
        "    FROM temperatures\n",
        "    WHERE Mean > 1.0\n",
        "    ORDER BY Mean DESC\n",
        "    LIMIT 15\n",
        "\"\"\")\n",
        "\n",
        "print(\"Months with temperature anomaly > 1.0°C:\")\n",
        "high_temps.show()"
      ],
      "metadata": {
        "id": "sql_where"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregation with GROUP BY\n",
        "yearly_stats = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        Year,\n",
        "        ROUND(AVG(Mean), 4) as avg_temp,\n",
        "        ROUND(MAX(Mean), 4) as max_temp,\n",
        "        ROUND(MIN(Mean), 4) as min_temp,\n",
        "        COUNT(*) as num_records\n",
        "    FROM temperatures\n",
        "    GROUP BY Year\n",
        "    HAVING AVG(Mean) > 0.5\n",
        "    ORDER BY avg_temp DESC\n",
        "\"\"\")\n",
        "\n",
        "print(\"Years with average temperature anomaly > 0.5°C:\")\n",
        "yearly_stats.show(20)"
      ],
      "metadata": {
        "id": "sql_groupby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregation by decade\n",
        "warmest_per_decade = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        Decade,\n",
        "        ROUND(AVG(Mean), 4) as Avg_Temp,\n",
        "        COUNT(*) as Num_Records,\n",
        "        MIN(Year) as First_Year,\n",
        "        MAX(Year) as Last_Year\n",
        "    FROM temperatures\n",
        "    GROUP BY Decade\n",
        "    ORDER BY Decade\n",
        "\"\"\")\n",
        "\n",
        "print(\"Average temperature anomaly by decade:\")\n",
        "warmest_per_decade.show()"
      ],
      "metadata": {
        "id": "sql_subquery"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Combining DataFrame API and SQL\n",
        "\n",
        "You can seamlessly switch between DataFrame API and SQL operations."
      ],
      "metadata": {
        "id": "combine_api_sql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with SQL, continue with DataFrame API\n",
        "recent_years = spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM temperatures\n",
        "    WHERE Year >= 2000\n",
        "\"\"\")\n",
        "\n",
        "# Continue with DataFrame operations\n",
        "monthly_pattern = recent_years \\\n",
        "    .groupBy(\"Month\") \\\n",
        "    .agg(\n",
        "        round(avg(\"Mean\"), 4).alias(\"Avg_Temp\"),\n",
        "        count(\"*\").alias(\"Count\")\n",
        "    ) \\\n",
        "    .orderBy(\"Month\")\n",
        "\n",
        "print(\"Monthly temperature patterns (2000+):\")\n",
        "monthly_pattern.show(12)"
      ],
      "metadata": {
        "id": "api_sql_combined"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The result of DataFrame operations can be registered as a new view\n",
        "monthly_pattern.createOrReplaceTempView(\"monthly_patterns\")\n",
        "\n",
        "# Query the new view\n",
        "warmest_months = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        Month,\n",
        "        Avg_Temp,\n",
        "        CASE\n",
        "            WHEN Month IN (12, 1, 2) THEN 'Winter'\n",
        "            WHEN Month IN (3, 4, 5) THEN 'Spring'\n",
        "            WHEN Month IN (6, 7, 8) THEN 'Summer'\n",
        "            ELSE 'Autumn'\n",
        "        END as Season\n",
        "    FROM monthly_patterns\n",
        "    ORDER BY Avg_Temp DESC\n",
        "\"\"\")\n",
        "\n",
        "warmest_months.show()"
      ],
      "metadata": {
        "id": "register_and_query"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 5: Practical Exercises\n",
        "---\n",
        "\n",
        "Now it's your turn to apply what you've learned! Complete the following exercises using the datasets provided."
      ],
      "metadata": {
        "id": "exercises_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Movie Ratings Analysis\n",
        "\n",
        "Let's work with a real dataset! We'll use the MovieLens 100K dataset, which contains 100,000 movie ratings."
      ],
      "metadata": {
        "id": "exercise1_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download MovieLens 100K dataset\n",
        "!wget -q https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
        "!unzip -q -o ml-100k.zip\n",
        "\n",
        "# Load the ratings data\n",
        "ratings_schema = StructType([\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"movie_id\", IntegerType(), True),\n",
        "    StructField(\"rating\", IntegerType(), True),\n",
        "    StructField(\"timestamp\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "ratings_df = spark.read \\\n",
        "    .option(\"delimiter\", \"\\t\") \\\n",
        "    .schema(ratings_schema) \\\n",
        "    .csv(\"ml-100k/u.data\")\n",
        "\n",
        "# Load movie titles\n",
        "movies_df = spark.read \\\n",
        "    .option(\"delimiter\", \"|\") \\\n",
        "    .csv(\"ml-100k/u.item\") \\\n",
        "    .select(\n",
        "        col(\"_c0\").cast(\"int\").alias(\"movie_id\"),\n",
        "        col(\"_c1\").alias(\"title\"),\n",
        "        col(\"_c2\").alias(\"release_date\")\n",
        "    )\n",
        "\n",
        "print(\"Ratings DataFrame:\")\n",
        "ratings_df.show(5)\n",
        "\n",
        "print(\"\\nMovies DataFrame:\")\n",
        "movies_df.show(5)\n",
        "\n",
        "# Register as views for SQL\n",
        "ratings_df.createOrReplaceTempView(\"ratings\")\n",
        "movies_df.createOrReplaceTempView(\"movies\")"
      ],
      "metadata": {
        "id": "exercise1_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.1: Basic Statistics\n",
        "\n",
        "Calculate and display:\n",
        "1. Total number of ratings\n",
        "2. Number of unique users\n",
        "3. Number of unique movies\n",
        "4. Average rating across all movies"
      ],
      "metadata": {
        "id": "task1_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "task1_1_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2: Rating Distribution\n",
        "\n",
        "Show the distribution of ratings (how many ratings of each value 1-5). Order by rating value."
      ],
      "metadata": {
        "id": "task1_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "task1_2_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.3: Top Rated Movies\n",
        "\n",
        "Find the top 10 highest-rated movies that have at least 100 ratings. Join with the movies table to show the movie title.\n",
        "\n",
        "**Hint:** You'll need to:\n",
        "1. Group ratings by movie_id\n",
        "2. Calculate average rating and count\n",
        "3. Filter for movies with ≥100 ratings\n",
        "4. Join with movies_df to get titles\n",
        "5. Order by average rating (descending)"
      ],
      "metadata": {
        "id": "task1_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "task1_3_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.4: SQL Challenge\n",
        "\n",
        "Write a SQL query to find the top 5 users who have rated the most movies. Show their user_id, number of ratings, and average rating they give."
      ],
      "metadata": {
        "id": "task1_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your SQL query here\n",
        "# result = spark.sql(\"\"\"\n",
        "#     -- Write your query here\n",
        "\n",
        "# \"\"\")\n",
        "# result.show()"
      ],
      "metadata": {
        "id": "task1_4_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: E-commerce Sales Analysis\n",
        "\n",
        "Now let's work with an e-commerce dataset to practice more complex operations."
      ],
      "metadata": {
        "id": "exercise2_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a synthetic e-commerce dataset\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from builtins import round as python_round\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Generate data\n",
        "categories = [\"Electronics\", \"Clothing\", \"Home & Garden\", \"Books\", \"Sports\"]\n",
        "products = {\n",
        "    \"Electronics\": [\"Laptop\", \"Phone\", \"Tablet\", \"Headphones\", \"Camera\"],\n",
        "    \"Clothing\": [\"T-Shirt\", \"Jeans\", \"Jacket\", \"Shoes\", \"Hat\"],\n",
        "    \"Home & Garden\": [\"Chair\", \"Table\", \"Lamp\", \"Plant\", \"Vase\"],\n",
        "    \"Books\": [\"Fiction\", \"Non-Fiction\", \"Technical\", \"Children\", \"Comics\"],\n",
        "    \"Sports\": [\"Football\", \"Tennis Racket\", \"Yoga Mat\", \"Dumbbells\", \"Bicycle\"]\n",
        "}\n",
        "regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n",
        "\n",
        "sales_data = []\n",
        "base_date = datetime(2024, 1, 1)\n",
        "\n",
        "for i in range(5000):\n",
        "    category = random.choice(categories)\n",
        "    product = random.choice(products[category])\n",
        "    date = base_date + timedelta(days=random.randint(0, 364))\n",
        "    quantity = random.randint(1, 10)\n",
        "    base_price = random.uniform(10, 500)\n",
        "    discount = random.choice([0.0, 0.0, 0.0, 0.1, 0.15, 0.2, 0.25])  # Use 0.0 instead of 0\n",
        "    price = python_round(base_price * (1 - discount), 2)\n",
        "    total = python_round(price * quantity, 2)\n",
        "\n",
        "    sales_data.append((\n",
        "        i + 1,\n",
        "        date.strftime(\"%Y-%m-%d\"),\n",
        "        category,\n",
        "        product,\n",
        "        random.choice(regions),\n",
        "        float(quantity),      # Ensure float\n",
        "        float(price),         # Ensure float\n",
        "        float(total),         # Ensure float\n",
        "        float(discount)       # Ensure float\n",
        "    ))\n",
        "\n",
        "# Create DataFrame\n",
        "sales_columns = [\"order_id\", \"order_date\", \"category\", \"product\", \"region\",\n",
        "                 \"quantity\", \"unit_price\", \"total\", \"discount\"]\n",
        "sales_df = spark.createDataFrame(sales_data, sales_columns)\n",
        "\n",
        "# Add month column\n",
        "from pyspark.sql.functions import substring\n",
        "sales_df = sales_df.withColumn(\"month\", substring(\"order_date\", 1, 7))\n",
        "\n",
        "print(\"E-commerce Sales Data:\")\n",
        "sales_df.show(10)\n",
        "\n",
        "print(f\"\\nTotal records: {sales_df.count()}\")\n",
        "\n",
        "# Register as view\n",
        "sales_df.createOrReplaceTempView(\"sales\")"
      ],
      "metadata": {
        "id": "exercise2_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.1: Sales by Category\n",
        "\n",
        "Calculate total sales, number of orders, and average order value for each category. Order by total sales (descending)."
      ],
      "metadata": {
        "id": "task2_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "task2_1_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2: Monthly Trend Analysis\n",
        "\n",
        "Analyze monthly sales trends:\n",
        "1. Calculate total sales per month\n",
        "2. Show results ordered by month\n",
        "3. Create a simple visualization of the trend"
      ],
      "metadata": {
        "id": "task2_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "task2_2_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.3: Regional Performance\n",
        "\n",
        "Using SQL, find:\n",
        "1. Total sales by region\n",
        "2. The best-selling product in each region\n",
        "\n",
        "**Hint:** For part 2, you might need a subquery or window function."
      ],
      "metadata": {
        "id": "task2_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your SQL queries here\n",
        "\n"
      ],
      "metadata": {
        "id": "task2_3_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.4: Discount Impact Analysis\n",
        "\n",
        "Analyze the relationship between discounts and sales:\n",
        "1. Group orders by discount level (0%, 10%, 15%, 20%, 25%)\n",
        "2. Calculate average order value and total quantity for each discount level\n",
        "3. What patterns do you observe?"
      ],
      "metadata": {
        "id": "task2_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "task2_4_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge Exercise (Optional)\n",
        "\n",
        "Combine what you've learned to complete this advanced analysis.\n",
        "\n",
        "Using the sales data, create a comprehensive report that includes:\n",
        "\n",
        "1. **Category Deep Dive**: For each category, find the month with highest sales\n",
        "2. **Pareto Analysis**: Identify products that contribute to 80% of total sales\n",
        "3. **Performance Metrics**: Calculate month-over-month growth rate for total sales\n",
        "\n",
        "**Bonus**: Create visualizations for your findings."
      ],
      "metadata": {
        "id": "challenge_exercise"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here - Challenge Exercise\n",
        "\n"
      ],
      "metadata": {
        "id": "challenge_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Cleanup\n",
        "---\n",
        "\n",
        "Always remember to stop your Spark session when you're done to release resources."
      ],
      "metadata": {
        "id": "cleanup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")"
      ],
      "metadata": {
        "id": "stop_spark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Summary\n",
        "---\n",
        "\n",
        "In this lab, you learned:\n",
        "\n",
        "1. **Apache Spark Fundamentals**\n",
        "   - What Spark is and why it's used for big data processing\n",
        "   - Spark's master-worker architecture\n",
        "   - How to set up and configure Spark in Google Colab\n",
        "\n",
        "2. **RDDs (Resilient Distributed Datasets)**\n",
        "   - Creating RDDs from collections and files\n",
        "   - Transformations vs Actions\n",
        "   - Lazy evaluation and its benefits\n",
        "   - Classic Word Count example\n",
        "\n",
        "3. **Spark DataFrames**\n",
        "   - Creating DataFrames from various sources\n",
        "   - DataFrame operations: select, filter, withColumn\n",
        "   - Aggregation and grouping\n",
        "   - Handling missing data\n",
        "\n",
        "4. **Spark SQL**\n",
        "   - Creating temporary views\n",
        "   - Writing SQL queries on DataFrames\n",
        "   - Combining DataFrame API with SQL\n",
        "\n",
        "5. **Practical Application**\n",
        "   - Working with real-world datasets (Global Temperature, MovieLens)\n",
        "   - Data analysis patterns and best practices"
      ],
      "metadata": {
        "id": "summary"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Additional Resources\n",
        "---\n",
        "\n",
        "- Apache Spark Documentation: https://spark.apache.org/docs/latest/\n",
        "- PySpark API Reference: https://spark.apache.org/docs/latest/api/python/\n",
        "- Spark SQL Guide: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
        "- Databricks Learning: https://www.databricks.com/learn\n",
        "- Spark: The Definitive Guide (Book): https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/"
      ],
      "metadata": {
        "id": "resources"
      }
    }
  ]
}